{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf7496d-7833-4ecf-bf7d-c404391c84a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Understanding Fully Convolutional Neural Networks\n",
    "\n",
    "Now that we have generated the coarsened, low-resolution datasets, we can now feed them as training data for our parameterized machine learning (ML) models. In this tutorial series, we will focus on one category of ML models, fully convolutional neural networks (FCNNs), though there are other strata of models that can be employed (and have been explored within the paper) including hybrid linear and symbolic regression using genetic programming. Before we begin running these models and making predictions, in this notebook, we will take the time to better our understanding of FCNNs including the initialization process, undegoing training sessions, data preparation, feature extraction and generating predictions.\n",
    "\n",
    "The code that we utilize for running parameterized ML models resides within this [repository](https://github.com/m2lines/pyqg_parameterization_benchmarks). Our main focus of interest is in the files `neural_networks.py` and `utils.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d92ca2-33f4-42b1-b20a-f70c442565dc",
   "metadata": {},
   "source": [
    "## Initializing and Instantiating FCNNs \n",
    "\n",
    "Before we can ultimately begin making predictions we need to first create and then train our parameterized FCNNs. Starting at a high level, within `neural networks.py`, there sits the `FCNNParameterization` class. We use this class to generate parameterized FCNN models on which we can train and make predictions. \n",
    "\n",
    "The class method `train_on()` takes in the dataset that the models will be initially trained on, the path to save the models to as well as the inputs and targets we are training on as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdaa227-0939-4dbd-adb1-a215d2644112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:244\n",
    "class FCNNParameterization(Parameterization):\n",
    "    \n",
    "# neural_networks.py:281:287\n",
    "@classmethod\n",
    "def train_on(cls, dataset, directory,\n",
    "        inputs=['q','u','v'], \n",
    "        targets=['q_subgrid_forcing'], # See {INSERT SECTION REFERENCE} for valid target values of sugrid forcing and flux\n",
    "        num_epochs=50,\n",
    "        zero_mean=True,\n",
    "        padding='circular', **kw): # Accepts values 'same', 'circuluar', or None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1c7e1-1d3b-4b0d-bfd1-6c0866f7ca46",
   "metadata": {},
   "source": [
    "We can also pass in arguments for additional parameters including the number of epochs, whether the final output layers should be constrained to have zero spatial mean when predicting the subgrid forcing target, and padding technique. This method creates two `FullyCNN` objects, one for each layer of the quasigeostrophic model on which we ran simulations on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260dc0e2-f626-48ba-aa02-2184ee34f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:289:299\n",
    "layers = range(len(dataset.lev))\n",
    "\n",
    "models = [\n",
    "    FullyCNN(\n",
    "        [(feat, zi) for feat in inputs for zi in layers],\n",
    "        [(feat, z) for feat in targets],\n",
    "        zero_mean=zero_mean,\n",
    "        padding=padding \n",
    "\n",
    "    ) for z in layers\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f6cf7-47fe-4dc2-900d-eb551ebd2d03",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d29a8-5f03-4911-a9a0-fa3ce9538939",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Upon initializing the models, the raw training data must be preprocessed and prepared in order to feed into the models for training. This is done by first extracting the relevant input and target values from the training dataset. Since the dataset is passed as an `xarray.Dataset` we must convert it into proper `numpy.ndarray` format to be able to put into the `FullyCNN` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6342eed-0997-4ad1-a7d9-3143478c6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:308:309\n",
    "X = model.extract_inputs(dataset)\n",
    "Y = model.extract_targets(dataset)\n",
    "\n",
    "# neural_networks.py:57:66\n",
    "def extract_vars(self, m, features, dtype=np.float32):\n",
    "    ex = FeatureExtractor(m)\n",
    "\n",
    "    arr = np.stack([\n",
    "        np.take(ex(feat), z, axis=-3) for feat, z in features\n",
    "    ], axis=-3)\n",
    "\n",
    "    arr = arr.reshape((-1, len(features), ex.nx, ex.nx))\n",
    "    arr = arr.astype(dtype)\n",
    "    return arr\n",
    "\n",
    "# utils.py:126:128\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Helper class for taking spatial derivatives and translating string\n",
    "    expressions into data. Works with either pyqg.Model or xarray.Dataset.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d81f6-238f-4817-acd3-19badab618a6",
   "metadata": {},
   "source": [
    "The above functions `extract_inputs()` and `extract_targets()` are wrappper functions of the method `extract_vars()` which creates a `FeatureExtractor` object from the dataset. This class works with `pyqg.Model` or `xarray.Dataset` as a helper class for taking spatial derivatives and translating string expressions into data. This object is then used in extracting the appropriate features from the dataset and reshaping these features from an `xarray.Dataset` format to a `numpy.ndarray` representation which can be passed into the model. The main function that carries this out is `extract_feature()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb3630-bef2-4e4a-94e0-94446936e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py:208:209\n",
    "def extract_feature(self, feature):\n",
    "    \"\"\"Evaluate a string feature, e.g. laplacian(advected(curl(u,v))).\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7e488-7770-4a6f-8f41-85d5fdf6051a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Standardization\n",
    "\n",
    "Now, upon extracting the relevant features from the inputs and targets of the training dataset, another preprocessing technique that is then applied on these features is standardization. This entails scaling data to fit a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a4032-21bf-45ed-a5db-649158c1cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:310\n",
    "model.fit(X, Y, num_epochs=num_epochs, **kw)\n",
    "\n",
    "# neural_networks.py:131:135\n",
    "def fit(self, inputs, targets, rescale=False, **kw):\n",
    "        if rescale or not hasattr(self, 'input_scale') or self.input_scale is None:\n",
    "            self.input_scale = ChannelwiseScaler(inputs)\n",
    "        if rescale or not hasattr(self, 'output_scale') or self.output_scale is None:\n",
    "            self.output_scale = ChannelwiseScaler(targets, zero_mean=self.is_zero_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab5222-e883-473a-843b-63124516ac22",
   "metadata": {},
   "source": [
    "The function `fit()` takes in, as parameters, the extracted feature values for the inputs and targets and other additional parameters including the number of epochs to train on and whether to rescale based on the passed in input and target values. Each `FullyCNN` object has an input scaler and output scaler in the form of `ChannelwiseScaler` objects, which inherits from its parent class, `BasicScaler`. The `ChannelwiseScalar` class calculates the mean, zero spatial mean if necessary, and standard deviation along each feature channel of the the inputs and targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832b052-ec60-4b5d-8fdc-fa7044443316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:199:209\n",
    "class ChannelwiseScaler(BasicScaler):\n",
    "    def __init__(self, x, zero_mean=False):\n",
    "        assert len(x.shape) == 4\n",
    "        if zero_mean:\n",
    "            mu = 0\n",
    "        else:\n",
    "            mu = np.array([x[:,i].mean()\n",
    "                for i in range(x.shape[1])])[np.newaxis,:,np.newaxis,np.newaxis]\n",
    "        sd = np.array([x[:,i].std()\n",
    "            for i in range(x.shape[1])])[np.newaxis,:,np.newaxis,np.newaxis]\n",
    "        super().__init__(mu, sd)\n",
    "\n",
    "# neural_networks.py:188\n",
    "class BasicScaler(object):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a91992-0597-4d33-8cd7-98b6ebcfc102",
   "metadata": {},
   "source": [
    "These objects also perform the standardization step on the data. This is done by calling `transform()` on the input and output scaler objects and passing in the input and target values, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc9a3f-5e9e-43b1-bd28-dc4df7b58e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:136:139\n",
    "train(self,\n",
    "      self.input_scale.transform(inputs),\n",
    "      self.output_scale.transform(targets),\n",
    "      **kw)\n",
    "\n",
    "# neural_networks.py:193:194\n",
    "def transform(self, x):\n",
    "    return (x - self.mu) / self.sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee7c5f-2622-4ed7-8f07-7cb9efda15bf",
   "metadata": {},
   "source": [
    "Lastly, the function calls `train()` in order to kick off the training session now that the training data has been preprocessed and prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf965c-22c4-401e-9859-c6590841562d",
   "metadata": {},
   "source": [
    "## Training FCNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94937e6a-8199-4457-b9d9-6da1cad43c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:222\n",
    "def train(net, inputs, targets, num_epochs=50, batch_size=64, learning_rate=0.001, device=None):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a8b6d0-4fcd-4a68-afba-f66c67f35bf6",
   "metadata": {},
   "source": [
    "The above function `train()` takes in the model and the preprocessed inputs and targets, as well additional parameters including the number of epochs to train over, the batch size, the learning rate, and device to specify whether memory will be loaded onto the GPU or CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc8339-a285-42bd-8a4c-39333e63199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:223:242\n",
    "if device is None:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(num_epochs/2), int(num_epochs*3/4), int(num_epochs*7/8)], gamma=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    for x, y in minibatch(inputs, targets, batch_size=batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        yhat = net.forward(x.to(device))\n",
    "        ytrue = y.to(device)\n",
    "        loss = criterion(yhat, ytrue)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "    print(f\"Loss after Epoch {epoch+1}: {epoch_loss/epoch_steps}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50382cdc-de1c-4aaf-aa27-321d85d7f697",
   "metadata": {},
   "source": [
    "## Saving and Loading FCNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01d3f5-fd8e-4f05-ae9a-500832cafef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:311:312\n",
    "model.save(os.path.join(directory, f\"models/{z}\"))\n",
    "trained.append(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
