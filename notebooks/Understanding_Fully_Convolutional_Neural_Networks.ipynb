{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf7496d-7833-4ecf-bf7d-c404391c84a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Understanding Fully Convolutional Neural Networks\n",
    "\n",
    "Now that we have generated coarsened, low-resolution datasets, we can now feed them as training data into our parameterized machine learning (ML) models and begin running predictions against unseen datasets. In this tutorial series, we will focus on one category of ML models, fully convolutional neural networks (FCNNs), though there are other strata of models that can be employed (and are explored within the paper) including hybrid linear and symbolic regression using genetic programming. Before we begin running these models and making predictions, in this notebook, we will take the time to better our understanding of FCNNs including the initialization process, data preprocessing, undergoing training sessions and generating predictions.\n",
    "\n",
    "The local code that we utilize for running parameterized ML models resides within this [repository](https://github.com/m2lines/pyqg_parameterization_benchmarks). Our main focus of interest is in the files `neural_networks.py` and `utils.py` located in `src/pyqg_parameterization_benchmarks`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d92ca2-33f4-42b1-b20a-f70c442565dc",
   "metadata": {},
   "source": [
    "## Initializing and Instantiating FCNNs \n",
    "\n",
    "Starting at a high level, within `neural networks.py`, there sits the `FCNNParameterization` class. We use this class to generate parameterized FCNN models on which we can train and make predictions. \n",
    "\n",
    "The function that sets up this initial creation is the class method `train_on()` which takes in the dataset that the models will be initially trained on, the path to save the models to as well as the inputs and targets we are training on as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdaa227-0939-4dbd-adb1-a215d2644112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:244\n",
    "class FCNNParameterization(Parameterization):\n",
    "    \n",
    "# neural_networks.py:281:287\n",
    "@classmethod\n",
    "def train_on(cls, dataset, directory,\n",
    "        inputs=['q','u','v'], \n",
    "        targets=['q_subgrid_forcing'], # See {INSERT SECTION REFERENCE} for valid target values of sugrid forcing and flux\n",
    "        num_epochs=50,\n",
    "        zero_mean=True,\n",
    "        padding='circular', **kw): # Accepts values 'same', 'circuluar', or None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1c7e1-1d3b-4b0d-bfd1-6c0866f7ca46",
   "metadata": {},
   "source": [
    "We can also pass in arguments for additional parameters including the number of epochs, whether the final output layers should be constrained to have zero spatial mean when predicting the subgrid forcing target, and padding technique. This method then creates two `FullyCNN` objects, one for each layer of the quasigeostrophic model on which we ran simulations on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260dc0e2-f626-48ba-aa02-2184ee34f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:289:299\n",
    "layers = range(len(dataset.lev))\n",
    "\n",
    "models = [\n",
    "    FullyCNN(\n",
    "        [(feat, zi) for feat in inputs for zi in layers],\n",
    "        [(feat, z) for feat in targets],\n",
    "        zero_mean=zero_mean,\n",
    "        padding=padding \n",
    "\n",
    "    ) for z in layers\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b74b7b-23eb-4cd2-9416-cf74cf0782b2",
   "metadata": {},
   "source": [
    "By default, each FCNN model has 8 fully convolutional layers (128 and 64 filters for the first two layers, respectively and 32 thereafter), ReLU activations, batch normalization after all immediate layers, and circular padding due to the periodicity of the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e4dd4-7137-4738-91ad-ae9cff3f3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:14:23\n",
    "class FullyCNN(nn.Sequential):\n",
    "    \"\"\"Pytorch class defining our CNN architecture, plus some helpers for\n",
    "    dealing with constraints and scaling.\"\"\"\n",
    "    def __init__(self, inputs, targets, padding='circular', zero_mean=True):\n",
    "        if padding is None:\n",
    "            padding_5 = 0\n",
    "            padding_3 = 0\n",
    "        elif padding in ['same', 'circular']:\n",
    "            padding_5 = 2\n",
    "            padding_3 = 1\n",
    "\n",
    "# neural_networks.py:35:42\n",
    "block1 = self._make_subblock(nn.Conv2d(n_in, 128, 5, padding=padding_5, **kw))\n",
    "block2 = self._make_subblock(nn.Conv2d(128, 64, 5, padding=padding_5, **kw))\n",
    "block3 = self._make_subblock(nn.Conv2d(64, 32, 3, padding=padding_3, **kw))\n",
    "block4 = self._make_subblock(nn.Conv2d(32, 32, 3, padding=padding_3, **kw))\n",
    "block5 = self._make_subblock(nn.Conv2d(32, 32, 3, padding=padding_3, **kw))\n",
    "block6 = self._make_subblock(nn.Conv2d(32, 32, 3, padding=padding_3, **kw))\n",
    "block7 = self._make_subblock(nn.Conv2d(32, 32, 3, padding=padding_3, **kw))\n",
    "conv8 = nn.Conv2d(32, n_out, 3, padding=padding_3)\n",
    "\n",
    "# neural_networks.py:54:55\n",
    "def _make_subblock(self, conv):\n",
    "    return [conv, nn.ReLU(), nn.BatchNorm2d(conv.out_channels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f6cf7-47fe-4dc2-900d-eb551ebd2d03",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d29a8-5f03-4911-a9a0-fa3ce9538939",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Upon initializing the models, the method `train_on()` then trains the newly created models on the training dataset that was passed in. However, the raw training data must be preprocessed and prepared in order to be fed into the models for training. This is done by first extracting the relevant input and target feature values from the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6342eed-0997-4ad1-a7d9-3143478c6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:308:309\n",
    "X = model.extract_inputs(dataset)\n",
    "Y = model.extract_targets(dataset)\n",
    "\n",
    "# neural_networks.py:57:66\n",
    "def extract_vars(self, m, features, dtype=np.float32):\n",
    "    ex = FeatureExtractor(m)\n",
    "\n",
    "    arr = np.stack([\n",
    "        np.take(ex(feat), z, axis=-3) for feat, z in features\n",
    "    ], axis=-3)\n",
    "\n",
    "    arr = arr.reshape((-1, len(features), ex.nx, ex.nx))\n",
    "    arr = arr.astype(dtype)\n",
    "    return arr\n",
    "\n",
    "# utils.py:126:128\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Helper class for taking spatial derivatives and translating string\n",
    "    expressions into data. Works with either pyqg.Model or xarray.Dataset.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d81f6-238f-4817-acd3-19badab618a6",
   "metadata": {},
   "source": [
    "The above functions `extract_inputs()` and `extract_targets()` are wrappper functions of the method `extract_vars()` which creates a `FeatureExtractor` object from the dataset. This class works with `pyqg.Model` or `xarray.Dataset` as a helper class for taking spatial derivatives and translating string expressions into data since we specified the inputs and targets of the parameterized FCNN as strings. This object is used in extracting the appropriate features from the dataset and reshaping these features from an `xarray.Dataset` format to a `numpy.ndarray` representation which can then be passed into the FCNN. The main function that carries this out is `extract_feature()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb3630-bef2-4e4a-94e0-94446936e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py:208:209\n",
    "def extract_feature(self, feature):\n",
    "    \"\"\"Evaluate a string feature, e.g. laplacian(advected(curl(u,v))).\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7e488-7770-4a6f-8f41-85d5fdf6051a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Standardization\n",
    "\n",
    "Now, upon extracting the relevant features from the inputs and targets of the training dataset, another preprocessing technique that is then applied on these features is standardization. This entails scaling data to fit a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a4032-21bf-45ed-a5db-649158c1cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:310\n",
    "model.fit(X, Y, num_epochs=num_epochs, **kw)\n",
    "\n",
    "# neural_networks.py:131:135\n",
    "def fit(self, inputs, targets, rescale=False, **kw):\n",
    "        if rescale or not hasattr(self, 'input_scale') or self.input_scale is None:\n",
    "            self.input_scale = ChannelwiseScaler(inputs)\n",
    "        if rescale or not hasattr(self, 'output_scale') or self.output_scale is None:\n",
    "            self.output_scale = ChannelwiseScaler(targets, zero_mean=self.is_zero_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab5222-e883-473a-843b-63124516ac22",
   "metadata": {},
   "source": [
    "The function `fit()` takes in, as parameters, the extracted feature values for the inputs and targets and other additional parameters including the number of epochs to train on and whether to rescale based on the input and target values that are passed in. Each `FullyCNN` model has an input scaler and output scaler in the form of `ChannelwiseScaler` objects. The `ChannelwiseScalar` class, which inherits from its parent class, `BasicScaler`,  calculates the mean, zero spatial mean if necessary, and standard deviation along each feature channel of the the inputs and targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832b052-ec60-4b5d-8fdc-fa7044443316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:199:209\n",
    "class ChannelwiseScaler(BasicScaler):\n",
    "    def __init__(self, x, zero_mean=False):\n",
    "        assert len(x.shape) == 4\n",
    "        if zero_mean:\n",
    "            mu = 0\n",
    "        else:\n",
    "            mu = np.array([x[:,i].mean()\n",
    "                for i in range(x.shape[1])])[np.newaxis,:,np.newaxis,np.newaxis]\n",
    "        sd = np.array([x[:,i].std()\n",
    "            for i in range(x.shape[1])])[np.newaxis,:,np.newaxis,np.newaxis]\n",
    "        super().__init__(mu, sd)\n",
    "\n",
    "# neural_networks.py:188\n",
    "class BasicScaler(object):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a91992-0597-4d33-8cd7-98b6ebcfc102",
   "metadata": {},
   "source": [
    "These scaler objects also perform the standardization step on the data. This is done by calling `transform()` on the input and output scaler objects and passing in the input and target values, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc9a3f-5e9e-43b1-bd28-dc4df7b58e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:136:139\n",
    "train(self,\n",
    "      self.input_scale.transform(inputs),\n",
    "      self.output_scale.transform(targets),\n",
    "      **kw)\n",
    "\n",
    "# neural_networks.py:193:194\n",
    "def transform(self, x):\n",
    "    return (x - self.mu) / self.sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee7c5f-2622-4ed7-8f07-7cb9efda15bf",
   "metadata": {},
   "source": [
    "Lastly, a call is made to the function `train()` in order to kick off the training session now that the training data has been preprocessed and prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf965c-22c4-401e-9859-c6590841562d",
   "metadata": {},
   "source": [
    "## Training FCNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94937e6a-8199-4457-b9d9-6da1cad43c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:222\n",
    "def train(net, inputs, targets, num_epochs=50, batch_size=64, learning_rate=0.001, device=None):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a8b6d0-4fcd-4a68-afba-f66c67f35bf6",
   "metadata": {},
   "source": [
    "The above function `train()` performs a training session on the FCNN using a sample dataset. It takes in the instance of the `FullyCNN` object as well as the preprocessed inputs and targets from the training dataset. There are additional parameters including the number of epochs to train over, the batch size, the learning rate, and device to specify whether memory will be loaded onto the GPU or CPU. These parameters can be adjusted to yield different variations of training settings. By default, the FCNNs are trained for 50 epochs over minibatches of 64 samples.\n",
    "\n",
    "Examining the code in this method, first, a check is made to see whether GPU resources are available, otherwise the device falls back onto the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac0f1f-f018-46c1-ab2c-fcac03b80a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:223:225\n",
    "if device is None:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aff79b-fe59-45ae-8f6a-5d26fa50823e",
   "metadata": {},
   "source": [
    "The learning algorithm employed during training is the Adam optimizer, an adaptive learning rate optimizer and a powerful tool for improving the accuracy and speed of neural networks. The scheduling technique used during training is MultiStepLR, which decays the learning based on number of epochs reaching specific milestones. Training is evaluated on a mean squared error (MSE) loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc8339-a285-42bd-8a4c-39333e63199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:226:228\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(num_epochs/2), int(num_epochs*3/4), int(num_epochs*7/8)], gamma=0.1)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09a6cb-7753-4b25-9d6a-6387dfcaf826",
   "metadata": {},
   "source": [
    "Each pass of the below loop performs one training epoch in which first a batch of training data is loaded by calling `minibatch()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958bc646-5daf-4412-8b8f-33de9cd94416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:211\n",
    "def minibatch(*arrays, batch_size=64, as_tensor=True, shuffle=True):\n",
    "\n",
    "# neural_networks.py:229:232\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    for x, y in minibatch(inputs, targets, batch_size=batch_size):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cddf094-602d-4df9-ba80-de31721c18c5",
   "metadata": {},
   "source": [
    "The optimizer's gradients are then zeroed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa42de-ef13-4308-999f-d798302887b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:233\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d5291-d175-45cf-a9b7-bacea2e5edac",
   "metadata": {},
   "source": [
    "Predictions from the FCNN model are made for this input batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd315f-f8a4-44ee-ad8d-ea895f521739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:234\n",
    "yhat = net.forward(x.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a5309-9fdd-4a7c-af97-5aaebef372aa",
   "metadata": {},
   "source": [
    "Calculations of the loss for that set of predictions against the actual labels on the training dataset and the backward gradients over the learning weights are made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a22d17-5779-4529-ab47-db5498055296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:235:237\n",
    "ytrue = y.to(device)\n",
    "loss = criterion(yhat, ytrue)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727730b4-701c-4aa8-a2ee-5db33cfe7a74",
   "metadata": {},
   "source": [
    "The optimizer then adjusts the model's learning weights based the gradients observed for the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d93d2a-8a5f-4b9a-8ae0-0e1a7335a3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:238\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9013bf-2f77-4e27-8c3b-435e6ea297cc",
   "metadata": {},
   "source": [
    "Finally, the loss data is gathered and then reported as an average per-batch loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0e0fb-0af7-41ac-b78a-c9b501b53fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:239:242     \n",
    "    epoch_loss += loss.item()\n",
    "    epoch_steps += 1\n",
    "print(f\"Loss after Epoch {epoch+1}: {epoch_loss/epoch_steps}\")\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50382cdc-de1c-4aaf-aa27-321d85d7f697",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving and Loading FCNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a2b903-d294-4092-8ea0-a7e26d877d84",
   "metadata": {},
   "source": [
    "Upon completing the inital training session on these models, they are then saved to the directory path that was originally specified and an instance of the trained parameterization is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01d3f5-fd8e-4f05-ae9a-500832cafef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_networks.py:311:312\n",
    "model.save(os.path.join(directory, f\"models/{z}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aece81-83aa-4e02-bda8-45063a3fa7bb",
   "metadata": {},
   "source": [
    "To load a saved FCNN paramaterization later for further training or to make more predictions on, we instantiate a `FCNNParameterization` object, passing in the directory path to where the parameterization is saved and then read the models saved there by calling the class method `load()` from `FullyCNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bf056-4d0c-4db7-bc35-23ad386e12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = FCNNParameterization('/home/jovyan/models/fcnn_qu_to_Sq2')\n",
    "\n",
    "# neural_networks.py:245:250\n",
    "def __init__(self, directory, models=None, **kw):\n",
    "    self.directory = directory\n",
    "    self.models = models if models is not None else [\n",
    "        FullyCNN.load(f, **kw)\n",
    "        for f in sorted(glob.glob(os.path.join(directory, \"models/*\")))\n",
    "    ]\n",
    "\n",
    "# neural_networks.py:163:164\n",
    "@classmethod\n",
    "def load(cls, path, set_eval=True, **kwargs):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b473c6e-0b4d-4fef-a445-5fbc227a9974",
   "metadata": {},
   "source": [
    "## Running Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f5acf-16d6-48a4-86ca-aefc5f33c2c2",
   "metadata": {},
   "source": [
    "Having now created and trained parameterized FCNN models, we can begin making predictions on them against held-out datasets of filtered and coarse-grained high resolution simulations. We will focus our scope within these tutorials to offline testing, though we can also observe performance through online testing and metrics. The below function `test_offline()` takes in a coarsened, low-resolution dataset and then predicts the subgrid forcing targets using the parameterization. The parameterization's predictions are then evaluated on a number of different online metrics including the coefficient of determination ($R^2$) and the Pearson correlation ($\\rho$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a67e795-c090-4831-8535-91a29f5fba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py:82:84\n",
    "def test_offline(self, dataset):\n",
    "        \"\"\"Evaluate the parameterization on an offline dataset,\n",
    "        computing a variety of metrics.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420b15a-3e0a-4798-84b6-a8669f7a7f47",
   "metadata": {},
   "source": [
    "The function then returns an `xarray.Dataset` object describing the predictions made by the parameterization is returned including a number of computed metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
